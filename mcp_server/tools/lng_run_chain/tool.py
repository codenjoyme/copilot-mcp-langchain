import mcp.types as types
from langchain.prompts import PromptTemplate
from mcp_server.state_manager import state_manager
from mcp_server.llm import llm
from langchain.chains import LLMChain, SequentialChain

async def tool_info() -> dict:
    """Returns information about the lng_run_chain tool."""
    return {
        "name": "lng_run_chain",
        "description": """Runs the chain of prompts and other tools. 
First prompt processes input text and creates a summary, then the second prompt 
provides recommendations based on that summary. 
This tool allows for sequential processing of text through multiple prompts, 
enabling complex workflows and interactions with the LLM.

**Parameters:**
- `input_text` (string, required): The input text to process and summarize.

**Example Usage:**
- Provide a text input to be processed and summarized.
- The system will return a summary and recommendations based on the processed text.

**Output:**
- The output will be a JSON object containing the summary and recommendations generated by the LLM.

This tool is useful for scenarios where you need to analyze text and generate actionable 
insights or summaries based on that analysis.
""",
        "schema": {
            "type": "object",
            "required": ["input_text"],
            "properties": {
                "input_text": {
                    "type": "string",
                    "description": "The input text to process and summarize",
                }
            }
        }
    }

async def run_tool(name: str, parameters: dict) -> list[types.Content]:
    """Runs the chain of prompts and other tools."""
    try:
        input_text = parameters.get("input_text", None)
        
        # Create first prompt template
        prompt1 = PromptTemplate(
            input_variables=["input_text"],
            template="Process the following text and create a summary: {input_text}",
        )
        chain1 = LLMChain(llm=llm, prompt=prompt1, output_key="summary")
        
        # Create second prompt template
        prompt2 = PromptTemplate(
            input_variables=["summary"],
            template="Based on the following summary, provide recommendations: {summary}",
        )
        chain2 = LLMChain(llm=llm, prompt=prompt2, output_key="recommendations")

        # Create a sequential chain to run both prompts in order
        sequential_chain = SequentialChain(
            chains=[chain1, chain2],
            input_variables=["input_text"],
            output_variables=["summary", "recommendations"],
            verbose=True
        )
        
        # Run the sequential chain with the input data
        result = sequential_chain({"input_text": input_text})
       
        # Extract the final output from the result
        summary = result.get("summary", "No summary generated.")
        recommendations = result.get("recommendations", "No recommendations generated.")
        
        # combine to json output
        output = f"{{'summary': '{summary}', 'recommendations': '{recommendations}'}}"
        
        # Prepare the response content
        return [types.TextContent(type="text", text=output)]
    except Exception as e:
        return [types.TextContent(type="text", text=f"Error using prompt template: {str(e)}")]
